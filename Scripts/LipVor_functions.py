############## DEPENDENCIES ##############
import numpy as np
import ast
import itertools
import matplotlib.pyplot as plt
from scipy.spatial import Voronoi, voronoi_plot_2d
from shapely.geometry import Polygon, Point
from matplotlib.patches import Circle
import plotly.graph_objects as go
from tqdm import tqdm
from neuralsens import partial_derivatives as ns
from neuralsens.partial_derivatives import calculate_second_partial_derivatives_mlp,calculate_first_partial_derivatives_mlp
from scipy.spatial import ConvexHull




# Load Julia
try:
    from juliacall import Main as jl
    jl.include("../Scripts//JuliaSrc/VorFunctions.jl")
except:
    import warnings
    warnings.warn("Julia could not be loaded. Julia options will not be available.", RuntimeWarning)

import torch



###############################################################################################
########################################## FUNCTIONS ##########################################
###############################################################################################

############################### GENERATE HYPERCUBE VERTICES
def generate_hypercube_vertices(intervals):
    """
    Generate vertices for a hypercube (n-dimensional cube) defined by the given intervals.

    Parameters:
    - intervals (list of tuples): A list of tuples representing the intervals for each dimension.
      Each tuple should contain two values: the minimum and maximum values for that dimension.

    Returns:
    - vertices (list of tuples): A numpy array where each tuple represents a vertex of the hypercube.
      The vertices are generated by taking all possible combinations of values within the specified intervals (cartesian product).

    This function generates vertices for a hypercube in n-dimensional space, where n is determined by the number of intervals provided.

    Note: The number of vertices is equal to 2^n, where n is the number of dimensions, and each dimension's range is defined by the corresponding interval.
    """
    # Generate all possible combinations of values within the intervals
    vertices = list(itertools.product(*intervals))
    return np.array(vertices)

""" EXAMPLE OF USE
# Example usage:
n = 3  # Number of variables
intervals = [(0, 1), (0, 1), (0, 1)]  # Example intervals for each variable

vertices = generate_hypercube_vertices(intervals)
for vertex in vertices:
    print(vertex)
"""

############################### GET FACES OF THE HYPERCUBE

def extract_hypercube_faces(vertices,limits):
    """
    Extract the faces of a hypercube defined by its vertices and limits.

    Parameters:
    - vertices (numpy.ndarray): An array containing the vertices of the hypercube.
    - limits (list of tuples): A list of tuples representing the intervals for each dimension.
      Each tuple should contain two values: the minimum and maximum values for that dimension.

    Returns:
    - faces (dict): A dictionary where keys are face labels, and values are numpy arrays representing the vertices of each face.

    This function extracts the faces of a hypercube defined by its vertices and limits. It uses the limits to identify and separate the vertices into different faces based on each dimension.

    Note: The input 'vertices' should be a numpy array, and 'limits' should be a list of tuples specifying the dimension intervals.
    EXAMPLE OF USE:
    intervals = [(0, 1), (0, 1), (0, 1)]
    faces = extract_hypercube_faces(vertices,intervals)
    faces
    """
    faces = dict()
    for i,limit in enumerate(limits):
        faces['x_{}{}'.format(i,limit[0])] = vertices[vertices[:,i]==limit[0]]
        faces['x_{}{}'.format(i,limit[1])] = vertices[vertices[:,i]==limit[1]]
    return faces


############################### GET THE NORMAL VECTOR OF A HYPERPLANE

""" 
Justification of the method:
P1) The centroid is subtracted to generate vectors that should be part of the plane.
P2) From the vector matrix, singular value decomposition (SVD) is used to obtain the kernel vectors.
P3) Since it is a hyperplane, there is only one vector in the kernel which is given by the last column of the matrix vh
"""
def get_normal_vector(hyperplane_points, centroid_hypercube):
    """
    Compute the normal vector of a hyperplane defined by a set of points and ensure it points outward from a hypercube centroid.

    Parameters:
        hyperplane_points (array-like): An array of points defining the hyperplane.
        centroid_hypercube (array-like): The centroid of the hypercube.

    Returns:
        array-like: The normal vector of the hyperplane, adjusted to point outward from the hypercube centroid.

    This function calculates the normal vector of a hyperplane defined by a set of points in `hyperplane_points`. 
    It first computes the centroid of these points and then determines the normal vector using singular value decomposition (SVD). 
    The function also ensures that the normal vector points outward from the hypercube centroid by comparing it to a vector from 
    the centroid to a point on the hyperplane.

    If the normal vector points inward, it is inverted to ensure it points outward. The adjusted normal vector is then returned.
    """
    centroid = np.mean(hyperplane_points, axis=0)
    _, _, vh = np.linalg.svd(hyperplane_points - centroid, full_matrices=True)
    normal_vector = vh[-1]
    
    point_on_face = hyperplane_points[0]  
    vector_to_face = point_on_face - centroid_hypercube
    
    dot_product = np.dot(normal_vector, vector_to_face)

    if dot_product < 0:
        normal_vector = -normal_vector
    
    return normal_vector


############################### GENERATE SYMMETRIC POINTS

"""
The formula for symmetric_point consists of two parts:

np.dot(normal_vector, point_vector) calculates the projection of point_vector onto normal_vector.
This projection represents the distance from the original point to the hyperplane in the direction of the normal vector.
Multiplying this distance by 2 means we are retracing twice that distance from the original point,
crossing the hyperplane and going an equal distance to the opposite side of the hyperplane.

Then, we subtract this distance from the original point to obtain the symmetric point.
By subtracting the distance in the direction of the normal vector, the new point is located on the opposite side of the hyperplane at an equal distance.
"""
def calculate_symmetric_points_faces(point, hyperrectangle_faces):
    """
    Calculate the symmetric points of a given point with respect to the faces of a hyperrectangle.

    Parameters:
    point (numpy.ndarray): The point for which to calculate the symmetric points.
    hyperrectangle_faces (dict): A dictionary where keys are face identifiers and values are arrays of vertices defining each face.

    Returns:
    list: A list of symmetric points.
    """
    # Initialization of the list of symmetric points
    symmetric_points = []
    # Concatenate all the faces in a single array
    faces_array = np.concatenate(list(hyperrectangle_faces.values()))
    # Drop the repeated points
    faces_array = np.unique(faces_array, axis=0)
    centroid_hypercube = np.mean(faces_array, axis=0)
    for face in hyperrectangle_faces.keys():
        hyperrectangle_vertices = hyperrectangle_faces[face]
        normal_vector = get_normal_vector(hyperrectangle_vertices, centroid_hypercube)
        # Convert the normal vector to a unit vector
        normal_vector = normal_vector / np.linalg.norm(normal_vector)
        point_vector = point - hyperrectangle_vertices[0] 
        # The symmetric point is calculated (Given a vector n, the symmetric vector is given by: v-2*(v.n)*n)
        symmetric_point = point - 2 * np.dot(point_vector, normal_vector) * normal_vector
        symmetric_points.append(symmetric_point)
    return symmetric_points


############################### CHECK IF A POINT IS INSIDE A HYPERCUBE

def is_inside_hypercube(point,hypercube_vertices):
    """
    Check if a point is inside a hypercube defined by its vertices.

    This function determines whether a given point resides inside a hypercube.
    A hypercube is a multi-dimensional cube, and its vertices are used to define its boundaries.
    The function checks if every coordinate of the input point falls within the boundaries
    of the hypercube. In cases where a coordinate is equal to the minimum or maximum
    value of the hypercube, it is still considered inside the hypercube.

    Parameters:
    point (array-like): The coordinates of the point to be checked for inclusion in the hypercube.
    hypercube_vertices (array-like): An array containing the vertices of the hypercube, 
                                    where each row represents a vertex and each column a coordinate.

    Returns:
    bool: True if the point is inside the hypercube, False otherwise.

    Examples:
    >>> point = [0.5, 0.5, 0.5]
    >>> hypercube_vertices = np.array([[0, 0, 0], [1, 1, 1]])
    >>> is_inside_hypercube(point, hypercube_vertices)
    True

    >>> point = [1.2, 0.5, 0.5]
    >>> hypercube_vertices = np.array([[0, 0, 0], [1, 1, 1]])
    >>> is_inside_hypercube(point, hypercube_vertices)
    False
    """
    ## Check if the point is inside the hypercube
    ## Check if every coordinate is between the minimum and maximum values of the hypercube
    ## In case of equality, the point is considered inside the hypercube
    ## If any coordinate is outside the hypercube, the point is considered outside the hypercube
    return np.all(np.logical_and(np.min(hypercube_vertices, axis=0) <= point, point <= np.max(hypercube_vertices, axis=0)))

############################### PROYECTION TO THE HYPERCUBE

def proyection_hypercube(point, hypercube):
    """
    Given a point and a hypercube, returns the projection of the point onto the hypercube.

    Args:
        point (numpy.ndarray): The point to be projected.
        hypercube (numpy.ndarray): The hypercube defined by its vertices.

    Returns:
        numpy.ndarray: The projected point.

    Example of use:
    n = 3  # Number of variables
    intervals = [(0, 1), (0, 2), (0, 0.5)]  # Example intervals for each variable

    vertices = utilities_voronoi.generate_hypercube_vertices(intervals)
    point = np.array([1.1,2.5,0.2]) 
    proyection_hypercube(point,vertices) -> (1,2.0,0.2)
    """
    # Check if the point is inside the hypercube
    if is_inside_hypercube(point, hypercube):
        # If the point is inside the hypercube, return it
        return point
    else:
        # If the point is outside the hypercube, project it onto the hypercube
        # The projection consists of reducing the coordinates that are outside the hypercube
        # to the minimum or maximum values of the hypercube

        # Extract the minimum and maximum coordinates of the hypercube
        min_coords = hypercube.min(axis=0)
        max_coords = hypercube.max(axis=0)

        # Iterate over each coordinate of the point
        projected_point = []
        for i in range(len(point)):
            # Check if the coordinate is within the hypercube
            if point[i] < min_coords[i]:
                projected_point.append(min_coords[i])
            elif point[i] > max_coords[i]:
                projected_point.append(max_coords[i])
            else:
                projected_point.append(point[i])

        # Return the projected point as a numpy array
        return np.array(projected_point)
    
def proyection_hypercube_vectorized(points, hypercube):
    """
    Given an array of points and a hypercube, returns the projection of each point onto the hypercube.

    Args:
        points (numpy.ndarray): The points to be projected. Each row represents a point.
        hypercube (numpy.ndarray): The hypercube defined by its vertices.

    Returns:
        numpy.ndarray: The projected points. Each row represents a projected point.

    Example of use:
    n = 3  # Number of variables
    intervals = [(0, 1), (0, 2), (0, 0.5)]  # Example intervals for each variable

    vertices = utilities_voronoi.generate_hypercube_vertices(intervals)
    points = np.array([[1.1,2.5,0.2], [0.5, 2.5, 0.1]]) 
    proyection_hypercube(points,vertices) -> array([[1,2.0,0.2], [0.5, 2.0, 0.1]])
    """
    # Extract the minimum and maximum coordinates of the hypercube
    min_coords = hypercube.min(axis=0)
    max_coords = hypercube.max(axis=0)

    # Initialize an empty list to store the projected points
    projected_points = []

    # Iterate over each point
    for point in points:
        # Check if the point is inside the hypercube
        if is_inside_hypercube(point, hypercube):
            # If the point is inside the hypercube, append it as is
            projected_points.append(point)
        else:
            # If the point is outside the hypercube, project it onto the hypercube
            # The projection consists of reducing the coordinates that are outside the hypercube
            # to the minimum or maximum values of the hypercube

            # Iterate over each coordinate of the point
            projected_point = []
            for i in range(len(point)):
                # Check if the coordinate is within the hypercube
                if point[i] < min_coords[i]:
                    projected_point.append(min_coords[i])
                elif point[i] > max_coords[i]:
                    projected_point.append(max_coords[i])
                else:
                    projected_point.append(point[i])

            # Append the projected point to the list of projected points
            projected_points.append(projected_point)

    # Return the projected points as a numpy array
    return np.array(projected_points)


############################### ADD SYMMETRIC POINTS

def add_symmetric_points(vor,vertices,intervals):

    ## Extract faces of the hypercube
    faces = extract_hypercube_faces(vertices,intervals)
    ## Lista para almacenar los puntos simétricos
    symmetric_points_list = []
    ## Iteramos en las regiones del voronoi
    for i, r in enumerate(vor.point_region):
        ## Extraemos la región para el punto i-ésimo
        region = vor.regions[r]
        
        ## Si la región contiene un -1 (region infinita) se añaden puntos simétricos
        if -1 in region:
            symmetric_points = calculate_symmetric_points_faces(vor.points[i], faces)
            symmetric_points_list.append(symmetric_points)
        ### Además comprueba si todos los vértices de la región están dentro del hipercubo
        ### Si no están dentro del hipercubo, se añaden puntos simétricos
        else:
            region_vertices = vor.vertices[region]
            if not is_inside_hypercube(region_vertices,vertices): ########################## CHEQUEAR QUE FUNCIONA BIEN CUANDO EN VEZ DE UN PUNTO SE PASAN MULTIPLES PUNTOS
                symmetric_points = calculate_symmetric_points_faces(vor.points[i], faces)
                symmetric_points_list.append(symmetric_points)

    ## Combine original and symmetric points
    all_points = np.vstack((vor.points, np.array(symmetric_points_list).reshape(-1, vertices.shape[1]))) ### GENERALIZAR PARA N DMIENSIONES
    return all_points, np.array(symmetric_points_list).reshape(-1, vertices.shape[1])

def check_space_filled_vectorized(vor, dict_radios, vertices):
    """
    Check if the space is filled by the Voronoi diagram within the given radius.

    Parameters:
    vor (scipy.spatial.Voronoi): The Voronoi diagram.
    dict_radios (dict): A dictionary where keys are points and values are radii.
    vertices (numpy.ndarray): The vertices of the hypercube.

    Returns:
    tuple: A tuple containing a boolean indicating if the space is filled and an array of distances.
    """
    distances = []
    # Extract radius values
    radius = np.array(list(dict_radios.values()))[:, 0]
    
    # Initialize the counter
    j = 0
    for i, point in enumerate(vor.points):
        region_idx = vor.point_region[i]
        if is_inside_hypercube(point, vertices):
            # Check that the distance is going to be calculated for the correct point
            # point_radio = np.array(ast.literal_eval(list(dict_radios.keys())[j]), dtype=np.float32)
            # if not np.allclose(point, point_radio, atol=1e-8):
            #     print('Point from voronoi set: ', point)
            #     print('Point from dict_radios: ', point_radio)
            #     raise ValueError('The points are not the same, and therefore the radius is not the correct one')
            region_vertices = vor.vertices[vor.regions[region_idx]]
            # Find the furthest vertex from the point
            furthest_vertex = region_vertices[np.argmax(np.linalg.norm(region_vertices - point, axis=1))]
            distance = np.linalg.norm(point - furthest_vertex)
            distances.append(distance)
            j += 1

    distances_array = np.array(distances)

    # Check if all distances are within the radius
    if np.all(distances_array < radius):
        return True, distances_array
    else:
        return False, distances_array


def compute_polytope_volume(vertices):
    hull = ConvexHull(vertices)
    return hull.volume

############################### CHECK IF A POINT IS INSIDE A HYPERCUBE
    
def points_inside_hypercube(points, vertices):
    """
    Returns the points and their indices that are inside the hypercube defined by the given vertices.

    Args:
        points (numpy.ndarray): An array of points.
        vertices (numpy.ndarray): An array of vertices defining the hypercube.

    Returns:
        Tuple[numpy.ndarray, numpy.ndarray]: A tuple containing the points and their indices that are inside the hypercube.
    """
    points_inside = np.array([point for i, point in enumerate(points) if is_inside_hypercube(point, vertices)])
    indices_inside = np.array([i for i, point in enumerate(points) if is_inside_hypercube(point, vertices)])
    return points_inside, indices_inside

############################### GET HESSIAN BOUND

def hessian_bound(W, actfunc, partial_monotonic_variable, n_variables):
    """
    Calculate the Hessian bound for a neural network with given weights and activation functions.

    Parameters:
    W (list of numpy.ndarray): List of weight matrices for each layer of the neural network.
    actfunc (list of str): List of activation functions for each layer of the neural network.
    partial_monotonic_variable (list of int): List of indices of the partial monotonic variables.
    n_variables (int): Number of variables in the input vector.

    Returns:
    float: The maximum Hessian bound.
    """
    ## In case there are multiple partial monotonic variables
    hessian_bounds = []
    for var in partial_monotonic_variable: 
        hessian_boud =[]
        ## First compute H_0^1
        ## Remember that ||H_0_1|| <= max|a_k_1|*||W_1_1||*||W_1|| 
        # where W_1_1 is the first row of the first layer of weights and a_k_1 is the maximum possible value of the second derivative of the activation function of the first layer
        
        ## Generate the vector (0,...,1(i),...,0)
        input_vector = [0] * n_variables
        input_vector[var] = 1

        if actfunc[1] == 'sigmoid':
            a_1 = 0.25
        else:
            a_1 = 1
        ## W_1j^1 * W_1
        weights_multiplication = np.linalg.norm(input_vector @ W[1][1:, :], ord=2) * np.linalg.norm(W[1][1:, :], ord=2)
        H_0_1 = a_1 * weights_multiplication
        hessian_boud.append(H_0_1)
        
        ## In general, H_0_k can be bounded following the next equation
        # H_0_k <= max|a_k|*||W_1_1||*||W_1||*(||W_2||^2)*...*(||W_{k+1}||^2) + H_0_{k-1}*||W_k||
        for k in range(2, len(actfunc)):
            if actfunc[k] == 'sigmoid':
                a_k = 0.25
            elif actfunc[k] == 'identity': 
                a_k = 0
            else:
                a_k = 1
            weights_multiplication = weights_multiplication * np.linalg.norm(W[k][1:, :], ord=2)**2
            H_0_k = a_k * weights_multiplication + hessian_boud[-1] * np.linalg.norm(W[k][1:, :], ord=2)
            hessian_boud.append(H_0_k)
        ## Return the last element of the list
        hessian_bounds.append(hessian_boud[-1])
    return max(hessian_bounds)

def get_weights_and_biases(model):
    """
    Retrieves the weights and biases from a PyTorch model.

    This function extracts the weights and biases from the layers of a PyTorch model.
    The weights and biases are extracted from the model's state dictionary and returned in two separate lists.

    Parameters:
    model (torch.nn.Module): The PyTorch model from which to extract weights and biases.

    Returns:
    tuple: A tuple containing two lists. The first list contains the weights from each layer of the model,
           and the second list contains the biases from each layer of the model.

    Example:
    >>> model = torch.nn.Linear(2, 3)
    >>> weights, biases = get_weights_and_biases(model)
    >>> print(weights)
    [tensor([[ 0.3643, -0.3121],
             [-0.1371,  0.3319],
             [-0.6657,  0.4241]])]
    >>> print(biases)
    [tensor([0.0925, 0.2071, 0.2133])]
    """
    parameters = dict(model.state_dict())
    weights = []
    biases = []
    for key, value in parameters.items():
        if 'weight' in key:
            weights.append(value.T)
        elif 'bias' in key:
            biases.append(value)
    return weights, biases

############################### GET LIPSCHITZ RADIUS

def get_lipschitz_radius_neuralsens(inputs, outputs, weights, biases, actfunc, global_lipschitz_constant, monotone_relation, variable_index, n_variables, epsilon_derivative):
    """
    Calculate the Lipschitz radius for a neural network with given inputs, outputs, weights, biases, and activation functions.

    Parameters:
    inputs (torch.Tensor): The input data.
    outputs (torch.Tensor): The output data.
    weights (list of torch.Tensor): List of weight matrices for each layer of the neural network.
    biases (list of torch.Tensor): List of bias vectors for each layer of the neural network.
    actfunc (list of str): List of activation functions for each layer of the neural network.
    global_lipschitz_constant (torch.Tensor): The global Lipschitz constant of the neural network.
    monotone_relation (list or torch.Tensor): List or tensor indicating the monotone relation for each variable.
    variable_index (list of int): List of indices of the variables to consider.
    n_variables (int): Number of variables in the input vector.
    epsilon_derivative (float): Threshold for the derivative to consider for retraining.

    Returns:
    tuple: A tuple containing the list of radii, a dictionary of radii, the retraining input tensor, and a boolean indicating if no points violate the monotone relation.
    """
    radius_tot = []
    dict_radios = {}
    x_reentrenamiento = torch.tensor([]).reshape(-1, n_variables)
    ## Assert that the length of monotone_relation is the same as the number of variables and the length of variable_index
    assert len(monotone_relation) == len(variable_index), "The length of the monotone relation and the variable index must be the same"
    ## Check if monotone_relation is a tensor
    if not isinstance(monotone_relation, torch.Tensor):
        monotone_relation = torch.tensor(monotone_relation).float()
    
    _, _, _, _, D_accum, _, _ = calculate_first_partial_derivatives_mlp(weights, biases, actfunc, inputs, outputs, sens_end_layer=len(actfunc))
    derivatives = D_accum[-1]
    no_points = True
    for i, der in enumerate(derivatives):
        x = inputs[i]
        derivative = torch.tensor(der.flatten()).float()[variable_index]
        if (derivative == 0).any():
            x_reentrenamiento = torch.cat((x_reentrenamiento, x.reshape(-1, n_variables)), dim=0)
            r = 0
            radius_tot.append(r)
            dict_radios[repr(list(x.detach().numpy()))] = [r, 0]
        elif torch.sum(torch.relu(-monotone_relation * derivative)) > 0:
            no_points = False  ## There is at least one point not satisfying the monotone relation
            x_reentrenamiento = torch.cat((x_reentrenamiento, x.reshape(-1, n_variables)), dim=0)
            derivative_neg = torch.relu(-monotone_relation * derivative)
            max_der = torch.max(derivative_neg).item()
            r = max_der / global_lipschitz_constant.item()
            radius_tot.append(r)
            dict_radios[repr(list(x.detach().numpy()))] = [r, -1]
        else:
            r = torch.min(monotone_relation * derivative).item() / global_lipschitz_constant.item()
            radius_tot.append(r)
            dict_radios[repr(list(x.detach().numpy()))] = [r, 1]

    ## When no_points is True it means that there are no points not satisfying the monotone relation
    if no_points:
        for i, der in enumerate(derivatives):
            x = inputs[i]
            derivative = torch.tensor(der.flatten()).float()[variable_index]
            if not (derivative == 0).any() and not torch.sum(torch.relu(-monotone_relation * derivative)) > 0:
                if torch.min(monotone_relation * derivative).item() < epsilon_derivative:
                    x_reentrenamiento = torch.cat((x_reentrenamiento, x.reshape(-1, n_variables)), dim=0)

    return radius_tot, dict_radios, x_reentrenamiento, no_points

############################### ADD POINTS TO THE VORONOI DIAGRAM
def add_new_point_vectorized(finite_vor, vertices, distances, dict_radios, probability):
    """
    Add a new point to the Voronoi diagram in a vectorized manner.

    Parameters:
    finite_vor (scipy.spatial.Voronoi): The Voronoi diagram.
    vertices (numpy.ndarray): The vertices of the hypercube.
    distances (numpy.ndarray): The distances from each point to the furthest vertex in its Voronoi cell.
    dict_radios (dict): A dictionary where keys are points and values are radii.
    probability (float): The probability to select the minimum radius vertex.

    Returns:
    tuple: A tuple containing the selected vertex, the volume covered, and the count of covered vertices.
    """
    min_covered_points = float('inf')
    max_radio = float('-inf')
    min_radio = float('inf')
    selected_vertex_max = None
    selected_vertex_min = None
    vertex_covered_count = 0  # Initialize the counter
    volume_covered = 0  # Initialize the volume of the polytope

    ## Extract radios values
    radios = np.array(list(dict_radios.values()))[:,0] ## The second column is the relation indicator

    points = finite_vor.points
    point_inside, indices_inside = points_inside_hypercube(points, vertices)

    assert len(indices_inside) == len(distances) == len(radios), "The number of points inside the voronoi set is not the same as the distances and the radios"
    ## Check that not all the radios are greater than the distances
    assert not np.all(radios >= distances), "The Voronoi cell is already filled"

    ## Loop over the distances and the radios
    for i, distance in enumerate(distances):
        ## Check if the distance is greater than the radio (If not the Voronoi cell is already filled)
        if distance > radios[i]:
            ## Extract the point and the region index
            point = finite_vor.points[indices_inside[i]]
            ## Check that the radio matches the point
            # point_radio = np.array(ast.literal_eval(list(dict_radios.keys())[i]),dtype=np.float32)
            # assert np.allclose(point, point_radio, atol=1e-8), "The points are not the same, and therefore the radio is not the correct one"
            
            ## Extract the region index and vertices for the point i
            region_idx = finite_vor.point_region[indices_inside[i]]
            region_vertices = finite_vor.vertices[finite_vor.regions[region_idx]]
            
            ## Extract the furthest vertex
            furthest_vertex = region_vertices[np.argmax(np.linalg.norm(region_vertices - point, axis=1))]

            # Precompute distances between point_inside and furthest_vertex
            distances_to_furthest = np.linalg.norm(point_inside - furthest_vertex, axis=1)

            # Count covered points using vectorized operations
            covered_points = np.sum(distances_to_furthest < radios)

            #if covered_points < min_covered_points:
            ## If the number of covered points is less than the minimum covered points or the number of covered points is the same but the radio is greater
            if covered_points < min_covered_points or (covered_points == min_covered_points and radios[i] < min_radio):
                    min_covered_points = covered_points
                    min_radio = radios[i]
                    selected_vertex_min = furthest_vertex
            if covered_points < min_covered_points or (covered_points == min_covered_points and radios[i] > max_radio):
                    min_covered_points = covered_points
                    max_radio = radios[i]
                    selected_vertex_max = furthest_vertex
        else:
            ## Extract the point and the region index
            point = finite_vor.points[indices_inside[i]]
            ## Check that the radio matches the point
            # point_radio = np.array(ast.literal_eval(list(dict_radios.keys())[i]),dtype=np.float32)
            # assert np.allclose(point, point_radio, atol=1e-8), "The points are not the same, and therefore the radio is not the correct one"
            
            ## Extract the region index and vertices for the point i
            region_idx = finite_vor.point_region[indices_inside[i]]
            region_vertices = finite_vor.vertices[finite_vor.regions[region_idx]]

            ## If the Voronoi cell is already filled, we increase the counter
            vertex_covered_count += 1
            ## If the voronoi cell is filled compute the volume of the polytope
            
            ##Project the points inside the hypercube
            #region_vertices = proyection_hypercube_vectorized(region_vertices, vertices)
            volume = compute_polytope_volume(region_vertices) 
            volume_covered += volume

    if np.random.rand() < probability:
        selected_vertex = selected_vertex_min
    else:
        selected_vertex = selected_vertex_max
    return selected_vertex, volume_covered, vertex_covered_count  # Return the counter along with the selected vertex

def LipVor(original_vor, original_points, finite_vor, dict_radios, vertices, distances, 
                          model,actfunc, global_lipschitz_constant, intervals,monotone_relations,variable_index,
                          n_variables, epsilon_derivative, probability, mode='neuralsens',plot_voronoi=False, epsilon=1e-5, max_iterations=10, verbose=0):
    """
    Add points to a Voronoi diagram using the furthest vertex for each point.

    Args:
        original_vor (scipy.spatial.Voronoi): The original Voronoi diagram.
        original_points (numpy.ndarray): The original points in the Voronoi diagram.
        finite_vor (scipy.spatial.Voronoi): The finite Voronoi diagram (with the added symmetric points)
        vertices (numpy.ndarray): The vertices defining the hypercube.
        distances (dict): The distances for each point in the Voronoi diagram.
        model (torch.nn.Module): The trained model.
        global_lipschitz_constant (float): The global Lipschitz constant.
        x_lim (tuple): The x-axis limits of the hypercube.
        y_lim (tuple): The y-axis limits of the hypercube.
        monotone_relations (list): The monotone relations for each variable.
        variable_index (list): The indices of the variables to compute the local Lipschitz constant.
        n_variables (int): The number of variables.
        plot_voronoi (bool, optional): Whether to plot each Voronoi diagram. Defaults to False.
        epsilon (float, optional): The extension of the hypercube. Defaults to 1e-5. It is needed to compute symmetric points on the boundary.
        max_iterations (int, optional): The maximum number of iterations. Defaults to 10.

    Returns:
        numpy.ndarray: The updated original points in the Voronoi diagram.
    """
    intervals_extended = [(x - epsilon, y + epsilon) for x, y in intervals]
    vertices_extended = generate_hypercube_vertices(intervals_extended)

    ## Boolean warning to print if there are points not following the monotone relation
    warning = False
    
    if mode == 'neuralsens':
    #     # print('Using NeuralSens')
        weights, biases = get_weights_and_biases(model)
    elif mode == 'autograd':
        print('Using autograd')
    else:
        raise ValueError('The mode must be either autograd or neuralsens')
    
    iteration_range = range(max_iterations)
    if verbose:
        iteration_range = tqdm(iteration_range, desc="Processing iterations")
    
    for i in iteration_range:
        if verbose:
            ## Set description of the pbar
            iteration_range.set_description("Processing iteration {} with intervals defining the space: {}".format(i+1, intervals))
        
        ## Add new point
        selected_vertex,volume_covered,vertex_covered_count = add_new_point_vectorized(finite_vor=finite_vor, vertices=vertices, distances=distances, dict_radios=dict_radios,probability=probability)
        
        if verbose:
            ## Show in the pbar the number of vertex covered out of the total number of vertices
            percentage_covered = (vertex_covered_count / len(distances)) * 100
            percentage_volume_covered = (volume_covered / compute_polytope_volume(vertices_extended)) * 100
            iteration_range.set_postfix({'Percentage of vertex covered': f'{percentage_covered:.2f}%', 'Percentage of Volume Verified': f'{percentage_volume_covered:.2f}%'})
        
        ## Project the new point to the hypercube (because of the extension it may be outside the hypercube)
        selected_vertex = proyection_hypercube(selected_vertex, vertices)

        ## Checks if selected vertex is already in the original points
        ## In that case the loop has to stop because the dictionary cannot have two arrays with the same key
        if np.any(np.all(np.isclose(original_points, selected_vertex,rtol=1e-05), axis=1)):
            print('The selected vertex is already in the original points and the vertex is {}'.format(selected_vertex))
            break
        else:
            ## Add the new point to the original points
            original_points = np.vstack((original_points, selected_vertex))
        ## Add the new point to the inputs
        inputs = torch.tensor(original_points, dtype=torch.float)
        
        # # Add the new point to the Voronoi diagram
        original_vor.add_points(selected_vertex.reshape(1, -1))
        # Compute the new finite Voronoi diagram with the new point
        # FIXME: Corregir para que no se necesite utilizar el original_vor si no que se calculen los simétricos para todos los puntos
        all_points, _ = add_symmetric_points(original_vor, vertices_extended, intervals_extended)
        finite_vor = Voronoi(all_points, incremental=True)

        finite_vor = Voronoi(all_points, incremental=True,qhull_options="Q12 QJ Qs Qc Qx")
        
        ## Compute the new radios for each point
        if mode=='autograd':
            # Raise an error if the autograd option is selected as it is deprecated
            raise ValueError('The autograd option is deprecated')
        elif mode=='neuralsens':
            radius_tot, dict_radios, x_reentrenamiento,no_points = get_lipschitz_radius_neuralsens(inputs=inputs, outputs=[], weights=weights, biases=biases, actfunc=actfunc, 
                                                                                        global_lipschitz_constant=global_lipschitz_constant, 
                                                                                        monotone_relation=monotone_relations, variable_index=variable_index, 
                                                                                        n_variables=n_variables,epsilon_derivative=epsilon_derivative)

        derivative_sign = [v[1] for _, v in dict_radios.items()]
        ## Plot Voronoi diagram
        if plot_voronoi and len(intervals) == 2:
            plot_finite_voronoi_2D(vor=finite_vor, all_points=finite_vor.points, original_points=original_points, radios=radius_tot, boundary=vertices, derivative_sign=derivative_sign, plot_symmetric_points=False)
        ## Check if the space is filled
        space_filled, distances = check_space_filled_vectorized(finite_vor, dict_radios, vertices)
        ## Check if the space is filled and there are no points not satisfying the monotone relation
        if space_filled and no_points:
            print('The space is filled: {} after {} iterations. Intervals that define the space: {}'.format(space_filled, i+1, intervals))
            return space_filled, x_reentrenamiento
        elif x_reentrenamiento.shape[0]!=0 and not warning and not no_points:
            print('The retraining set is not empty and therefore the space cannot be filled')
            warning = True

    if len(intervals) == 2:
        plot_finite_voronoi_2D(vor=finite_vor, all_points=finite_vor.points, original_points=original_points, radios=radius_tot, boundary=vertices, derivative_sign=derivative_sign, plot_symmetric_points=False)

    return space_filled, x_reentrenamiento

def LipVor_Julia(original_points, radius, distances, vertices,
                          model,actfunc, global_lipschitz_constant, intervals,monotone_relations,variable_index,
                          n_variables, epsilon_derivative, epsilon_proyection, probability, mode='neuralsens',plot_voronoi=False,
                          n=1, max_iterations=10, verbose=0):

   
    ## Boolean warning to print if there are points not following the monotone relation
    warning = False
    
    if mode == 'neuralsens':
    #     # print('Using NeuralSens')
        weights, biases = get_weights_and_biases(model)
    elif mode == 'autograd':
        print('Using autograd')
    else:
        raise ValueError('The mode must be either autograd or neuralsens')
    
    iteration_range = range(max_iterations)
    if verbose:
        iteration_range = tqdm(iteration_range, desc="Processing iterations")

    # Compute the center as the mid point of the intervals
    # center = np.mean(intervals, axis=1)
    # # Compute the lenght of the inverval (Asumming cuboid domains)
    # # l = float(np.abs(intervals[0][1]-intervals[0][0]))
    # l = np.float64(np.abs(intervals[0][1] - intervals[0][0]))*(1+1e-4)
    intervals_extended = [(x - epsilon_proyection, y + epsilon_proyection) for x, y in intervals]
    center = np.mean(intervals_extended, axis=1)
    l = float(np.abs(intervals_extended[0][1] - intervals_extended[0][0]))

    # Compute the voronoi diagram
    # node_list,vertex_list = jl.generate_voronoi_nodes_points(original_points, l, center, plot_voronoi)
    node_list,vertex_list = jl.JuliaVoronoi(original_points, l, center, plot_voronoi)
    
    for i in iteration_range:
        # if verbose:
        #     ## Set description of the pbar
        #     iteration_range.set_description("Processing iteration {} with intervals defining the space: {}".format(i+1, intervals))
        
        ## Add new point
        # selected_vertex,volume_covered,vertex_covered_count = add_new_point_vectorized(finite_vor=finite_vor, vertices=vertices, distances=distances, dict_radios=dict_radios,probability=probability)
        # selected_vertex_julia = jl.add_new_point_julia(node_list, vertex_list, distances, radius, probability)
        selected_vertex_julia = jl.add_n_new_points_julia(node_list, vertex_list, distances, radius, probability,n)
        selected_vertex_julia = np.array([vertex for vertex in selected_vertex_julia]).squeeze()
        selected_vertex = np.array(selected_vertex_julia)
        if n==1:
            selected_vertex = proyection_hypercube(selected_vertex, vertices)
        else:
            selected_vertex = proyection_hypercube_vectorized(selected_vertex, vertices)

        # Add the selected_vertex to originial_points
        # FIXME: Es necesario hacer un transpose ya que para julia entran datos de dimension (dim,N) no (N,dim)
        original_points_before = original_points.shape[0]
        original_points = np.vstack((original_points, selected_vertex))

        # Get only unique values
        original_points = np.unique(original_points, axis=0)

        # Check if the number of points remains the same
        if original_points.shape[0] == original_points_before:
            print("Warning: No new node can be added given the current precision.")
            break

        # Compute the voronoi diagram
        try:
            # node_list,vertex_list = jl.generate_voronoi_nodes_points(original_points, l, center, plot_voronoi)
            node_list,vertex_list = jl.JuliaVoronoi(original_points, l, center, plot_voronoi)
        except:
            raise ValueError('Unable to generate the voronoi diagram')
    
        ## Add the new point to the inputs
        inputs = torch.tensor(original_points, dtype=torch.float)
        
        
        ## Compute the new radios for each point
        if mode=='autograd':
            # Raise an error if the autograd option is selected as it is deprecated
            raise ValueError('The autograd option is deprecated')
        elif mode=='neuralsens':
            radius, _, x_reentrenamiento,no_points = get_lipschitz_radius_neuralsens(inputs=inputs, outputs=[], weights=weights, biases=biases, actfunc=actfunc, 
                                                                                        global_lipschitz_constant=global_lipschitz_constant, 
                                                                                        monotone_relation=monotone_relations, variable_index=variable_index, 
                                                                                        n_variables=n_variables,epsilon_derivative=epsilon_derivative)

       ## Check if the space is filled
        # space_filled, distances = check_space_filled_vectorized(finite_vor, dict_radios, vertices)
        space_filled, distances, _ = jl.check_space_filled_julia(node_list,radius,vertex_list)
        ## Check if the space is filled and there are no points not satisfying the monotone relation
        if space_filled and no_points:
            print('The space is filled: {} after {} iterations. Intervals that define the space: {}'.format(space_filled, i+1, intervals))
            return space_filled, x_reentrenamiento
        elif x_reentrenamiento.shape[0]!=0 and not warning and not no_points:
            print('The retraining set is not empty and therefore the space cannot be filled')
            warning = True
    
    # Plot the final iteration 
    if len(intervals) == 2:
        # node_list,vertex_list = jl.generate_voronoi_nodes_points(original_points, l, center, True)
        node_list,vertex_list = jl.JuliaVoronoi(original_points, l, center, True)


    return space_filled, x_reentrenamiento

def find_missing_elements(subset_array, full_array, tol=1e-5):
    """
    Given a subset_array and a full_array (both as NumPy arrays), 
    returns the elements in full_array that are not in subset_array, 
    considering elements equal if they are closer than a given tolerance.

    Args:
        subset_array (numpy.ndarray): A subset of full_array.
        full_array (numpy.ndarray): The full array containing all elements.
        tol (float): Tolerance for considering elements as equal. Default is 1e-5.

    Returns:
        numpy.ndarray: Elements in full_array that are not in subset_array.
    """

    # Ensure the arrays are 2D for consistent processing
    if subset_array.ndim == 1:
        subset_array = subset_array.reshape(-1, 1)
    if full_array.ndim == 1:
        full_array = full_array.reshape(-1, 1)
    
    # Initialize a list to store missing elements
    missing_elements = []

    # Iterate over each element in full_array
    for full_elem in full_array:
        # Check if there is any element in subset_array that is close to full_elem
        if not np.any(np.all(np.abs(subset_array - full_elem) < tol, axis=1)):
            missing_elements.append(full_elem)
    
    # Convert the result back to a NumPy array
    result = np.array(missing_elements)
    
    return result

def plot_finite_voronoi_2D(vor, all_points, original_points, radios, boundary, derivative_sign, plot_symmetric_points=False):
    """
    Plot a 2D Voronoi diagram with finite regions.

    Parameters:
    vor (scipy.spatial.Voronoi): The Voronoi diagram.
    all_points (numpy.ndarray): All points in the Voronoi diagram.
    original_points (numpy.ndarray): The original points used to generate the Voronoi diagram.
    radios (numpy.ndarray): The radii for each point.
    boundary (numpy.ndarray): The boundary of the hypercube.
    derivative_sign (numpy.ndarray): The sign of the derivative for each point.
    plot_symmetric_points (bool): Whether to plot symmetric points. Default is False.
    """
    fig, ax = plt.subplots(figsize=(8, 8))
    # Define the boundary polygon
    if not np.array_equal(boundary[0], boundary[-1]):
       boundary = np.vstack((boundary, boundary[0]))
    boundary_polygon = Polygon(boundary)

    # Plot Voronoi regions
    if plot_symmetric_points:
        for region in vor.regions:
            if -1 not in region and len(region) > 0:
                polygon = [vor.vertices[i] for i in region]
                polygon_draw = Polygon(polygon)
                x, y = polygon_draw.exterior.xy
                plt.plot(x, y, color='k')
    else:
        for i, region in enumerate(vor.regions):
            if -1 not in region and len(region) > 0:
                point_index = np.where(vor.point_region == i)[0][0]
                point = vor.points[point_index]
                if is_inside_hypercube(point, boundary):
                    polygon = [vor.vertices[i] for i in region]
                    polygon_draw = Polygon(polygon)
                    x, y = polygon_draw.exterior.xy
                    plt.plot(x, y, color='k')

    # Find furthest vertices and draw dashed lines for original points
    for i in range(len(all_points)):
        region_idx = vor.point_region[i]
        region = vor.regions[region_idx]
        if is_inside_hypercube(all_points[i], boundary):
            region_vertices = vor.vertices[vor.regions[region_idx]]
            furthest_vertex = region_vertices[np.argmax(np.linalg.norm(region_vertices - all_points[i], axis=1))]
            plt.plot([all_points[i][0], furthest_vertex[0]], [all_points[i][1], furthest_vertex[1]], linestyle='--', color='purple')

    x, y = boundary_polygon.exterior.xy
    if plot_symmetric_points:
        plt.plot(all_points[:, 0], all_points[:, 1], 'go')

    # Add the radii to the plot
    for i in range(len(original_points)):
        if derivative_sign[i] == 1:
            plt.gca().add_patch(Circle(original_points[i], radios[i], color='b', alpha=0.25))
        else:
            plt.gca().add_patch(Circle(original_points[i], radios[i], color='r', alpha=0.25))

    mono_points = [i for i, sign in enumerate(derivative_sign) if sign == 1]
    non_mono_points = [i for i, sign in enumerate(derivative_sign) if sign == -1]
    plt.plot(original_points[mono_points, 0], original_points[mono_points, 1], 'b.', markersize=8, label='Partial Monotonic Points')
    plt.plot(original_points[non_mono_points, 0], original_points[non_mono_points, 1], 'ro', markersize=8, label='Non-Monotonic Points')
    if plot_symmetric_points:
        plt.title('Voronoi Diagram with Symmetric Points')
        plt.xlim(vor.min_bound[0] - 0.2, vor.max_bound[0] + 0.2)
        plt.ylim(vor.min_bound[1] - 0.2, vor.max_bound[1] + 0.2)
    else:
        plt.title('Voronoi Diagram')
        plt.xlim(min(x) - 0.2, max(x) + 0.2)
        plt.ylim(min(y) - 0.2, max(y) + 0.2)

    plt.gca().set_aspect('equal', adjustable='box')
    plt.legend(prop={'size': 12})
    plt.show()


def ms(x, y, z, radius, resolution=20):
    """
    Return the coordinates for plotting a sphere centered at (x, y, z).

    Parameters:
    x (float): X-coordinate of the center.
    y (float): Y-coordinate of the center.
    z (float): Z-coordinate of the center.
    radius (float): Radius of the sphere.
    resolution (int): Resolution of the sphere. Default is 20.

    Returns:
    tuple: A tuple containing the X, Y, and Z coordinates of the sphere.
    """
    u, v = np.mgrid[0:2*np.pi:resolution*2j, 0:np.pi:resolution*1j]
    X = radius * np.cos(u) * np.sin(v) + x
    Y = radius * np.sin(u) * np.sin(v) + y
    Z = radius * np.cos(v) + z
    return (X, Y, Z)


def plot_finite_voronoi_3D(vor, all_points, original_points, radios, vertices, plot_symmetric_points=False):
    """
    Plot a 3D Voronoi diagram with finite regions.

    Parameters:
    vor (scipy.spatial.Voronoi): The Voronoi diagram.
    all_points (numpy.ndarray): All points in the Voronoi diagram.
    original_points (numpy.ndarray): The original points used to generate the Voronoi diagram.
    radios (numpy.ndarray): The radii for each point.
    vertices (numpy.ndarray): The vertices of the hypercube.
    plot_symmetric_points (bool): Whether to plot symmetric points. Default is False.
    """
    polygons = []
    radii = np.random.rand(original_points.shape[0])
    for region_index, region in enumerate(vor.regions):
        if not -1 in region and len(region) > 0:
            region_vertices = vor.vertices[region]
            voronoi_point = vor.points[vor.point_region == region_index][0]
            if is_inside_hypercube(voronoi_point, vertices):
                region_vertices = [vor.vertices[i] for i in region]
                polygons.append(go.Mesh3d(x=[v[0] for v in region_vertices],
                                          y=[v[1] for v in region_vertices],
                                          z=[v[2] for v in region_vertices],
                                          opacity=0.1, alphahull=0, colorscale='Viridis'))

    if plot_symmetric_points:
        scatter = go.Scatter3d(x=all_points[:, 0], y=all_points[:, 1], z=all_points[:, 2], mode='markers', marker=dict(size=5))
    scatter_original = go.Scatter3d(x=original_points[:, 0], y=original_points[:, 1], z=original_points[:, 2], mode='markers', marker=dict(size=5))

    data_sph = []
    for i, point in enumerate(original_points):
        x_pns_surface, y_pns_surface, z_pns_suraface = ms(point[0], point[1], point[2], radios[i])
        data_sph.append(go.Surface(x=x_pns_surface, y=y_pns_surface, z=z_pns_suraface, colorscale='reds', opacity=0.2, showscale=False))

    if plot_symmetric_points:
        fig = go.Figure(data=[scatter] + [scatter_original] + polygons)
    else:
        fig = go.Figure(data=[scatter_original] + polygons + data_sph)

    x_min, x_max = np.min(original_points[:, 0]) - 1, np.max(original_points[:, 0]) + 1
    y_min, y_max = np.min(original_points[:, 1]) - 1, np.max(original_points[:, 1]) + 1
    z_min, z_max = np.min(original_points[:, 2]) - 1, np.max(original_points[:, 2]) + 1

    fig.update_layout(scene=dict(xaxis=dict(range=[x_min, x_max]),
                                 yaxis=dict(range=[y_min, y_max]),
                                 zaxis=dict(range=[z_min, z_max]),
                                 aspectmode="cube"),
                      height=1000, width=800)

    fig.show()
